{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49970cda-656f-4888-b2c9-9e0cd52a76ca",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"7\" >Web scraping</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788eba21-c927-47d3-8d0a-8f34aa31a399",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this simple project, I propose a classic way to gather the data from the web. I use a free web site of vehicle advertisements in France [**La centrale**](https://www.lacentrale.fr/) to gather the annonyme data of vehicles.\n",
    "\\\n",
    "\\\n",
    "**Important remark**\n",
    "\\\n",
    "When I made this project, the weeb site \"La Central\" didn't verify proptly if there is a robot or not. In the future the website can introduce a robot verification and the below code will not woork correctely. \n",
    "\\\n",
    "Actually the website ask a verification as you can see below, but the weeb scraping work very well anyway, so I think that this verification isn't efficace.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <a>\n",
    "    <img src=\"img/img2.PNG\"  >\n",
    "    </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18be3b-3db0-4a55-ad92-f2cd1f43d19b",
   "metadata": {},
   "source": [
    "# Calling the necessary libraries\n",
    "**Reminder** : you can easily install a new library in Jupyter by the command **!pip install name_of_labrary**. \\\n",
    "Example: \"!pip install bs4\". for more information please please see the [link](https://pypi.org/project/bs4/) of bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0266d9-eac2-4187-ad35-1b82ac3f4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Importing libraries #############################\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ba938-fb63-467f-8325-d889b081c3b4",
   "metadata": {},
   "source": [
    "**Initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4776d0-a0f1-4e60-95cc-12546ac44ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "df=pd.DataFrame(columns=['page','href','model','version','price','dep','prof','warranty','year','km'])\n",
    "Nb=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efed45f-e314-444f-be29-02e60d63ab14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The loop of each page of the website\n",
    "\n",
    "#### the main loop \"for i in range (1,501)\"\n",
    "\n",
    "The below part is the main part of the project. The main loop change the page index each time, so it open the page 1 and 2 and soone. We can change manually the page number \"1\" in the link 'https://www.lacentrale.fr/listing?page=1' by the number 2 or other number.\\\n",
    "I remark manually that the maximum number of pages is 500, because of that the for loop change the index \"i\" from 1 to 500 (see below).\n",
    "\\\n",
    "\\\n",
    "In this loop, the programme open the html file by **urlopen** and extract data from HTML by using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfb576-2a7d-4cba-9cdd-32d3ecc80ad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### the second loop \"for d in divs\":\n",
    "We can remark manually that each page shows 15 or 16 vehicles. This loop extract the data from each vehicle in the same page. especially the link of each vehicle by the command href='https...\n",
    "We can find data of each vehicle page in soup2. and the rest of code will read the needed data by using **find_all** command.\n",
    "\\\n",
    "\\\n",
    "To know which data you must read and how you can click on the **Inspection** by the right mouse click in the needed area, see the screenshot below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8b701-68d3-4f87-bef5-7d209bfd480a",
   "metadata": {},
   "source": [
    "<img src=\"img/img1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a3b800-abb3-4468-8cd4-bf8117b568fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 / 500 Nb vehicles 32\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,501): ########## Looping for each weebsite pages, the weebsite show 500 pages as maximum, 16 vehicles by page #########\n",
    "    try:\n",
    "        ################### Opening each page\n",
    "        url=r'https://www.lacentrale.fr/listing?page='+str(i)\n",
    "        userAgent = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36\"\n",
    "        req = Request(url, None, {'User-Agent': userAgent})\n",
    "        html = urlopen(req).read()\n",
    "        soup= BeautifulSoup(html,\"html.parser\")\n",
    "        divs = soup.find_all(\"div\", {\"class\": \"searchCard__rightContainer\"})\n",
    "    except:\n",
    "        break  ############ Break if the page didn't open  ######################\n",
    "    for d in divs:\n",
    "        try:\n",
    "            ################# gatherig the data from each page  ###################\n",
    "            href='https://www.lacentrale.fr'+d.parent.parent['href']\n",
    "            model=d.h3.find_all('span', {\"class\": \"searchCard__makeModel\"})[0].text\n",
    "            version=d.h3.find_all('span', {\"class\": \"searchCard__version\"})[0].text\n",
    "            price=d.div.div.select('div[class^=\"searchCard__fieldPrice\"]')[0].find_all('span')[-1].text\n",
    "            dep=d.select('div[class^=\"searchCard__customerLocalisation\"]')[0].select('div[class^=\"searchCard__dptCont\"]')[0].text\n",
    "            prof=d.select('div[class^=\"searchCard__customerLocalisation\"]')[0].select('div[class^=\"cbm-txt--default searchCard__customer\"]')[0].text\n",
    "            warranty=d.select('div[class^=\"searchCard__warrantyYearMileage\"]')[0].select('div[class^=\"searchCard__warranty\"]')[0].span.text\n",
    "            year=d.select('div[class^=\"searchCard__warrantyYearMileage\"]')[0].select('div[class^=\"searchCard__year\"]')[0].span.text\n",
    "            km=d.select('div[class^=\"searchCard__warrantyYearMileage\"]')[0].select('div[class^=\"searchCard__mileage\"]')[0].span.text\n",
    "            dic={'page':i,'href':href,'model':model,'version':version,'price':price,'dep':dep,\\\n",
    "                            'prof':prof,'warranty':warranty,'year':year,'km':km}\n",
    "            ######################################## Open the link of each vehicle ##########################\n",
    "            req2 = Request(href, None, {'User-Agent': userAgent})\n",
    "            html2 = urlopen(req2).read()\n",
    "            soup2= BeautifulSoup(html2,\"html.parser\")\n",
    "            div2= soup2.find_all(\"div\", {\"class\": \"cbm-moduleInfos__informationList cbm-moduleInfos__information_column_break\"})\n",
    "            ul=div2[0].find_all('ul')\n",
    "            #################### gathering data from link of each vehicle ######################\n",
    "            for u in ul:\n",
    "                li=u.find_all('li')\n",
    "                for l in li:\n",
    "                    span=l.find_all('span')\n",
    "                    dic[span[0].text]=span[1].text\n",
    "                    \n",
    "            myli = soup2.find_all(\"li\", {\"class\": \"list-item\"})\n",
    "            for each in myli:\n",
    "                strr=each.span.get_text()\n",
    "                if 'Volume du coffre :' in strr or 'Longueur :'in strr :\n",
    "                    strr=strr.split(':')\n",
    "                    dic[strr[0]]=strr[1]\n",
    "                    \n",
    "            for myspan in soup2.find_all(\"span\", {\"class\": \"headerSection-extraContent\"}):\n",
    "               dic['Ref_annonce']=myspan.get_text().split('Réf. annonce : ')[1]\n",
    "\n",
    "            for a in soup2.find_all(\"a\", {\"class\": \"link button-theme4\"}):\n",
    "                if 'équipements & options' in a.get_text():\n",
    "                    dic['Nb_option']=a.get_text().split('équipements & options')[0]\n",
    "            \n",
    "            publication=soup2.find_all(\"div\", {\"class\": \"cbm-toolboxButtons\"})[0].span.text.split('\\n')[1]\n",
    "            dic['publication']=publication\n",
    "            \n",
    "            ######################## Add data to the dataframe \n",
    "            df = df.append(dic,ignore_index=True)\n",
    "            Nb=Nb+1\n",
    "        except:pass\n",
    "\n",
    "    print(\"Page\",i,\"/ 500 Nb vehicles\",Nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b8bd3e-c2e2-4b7f-8155-d651ba1a9518",
   "metadata": {},
   "source": [
    "\\\n",
    "\\\n",
    "This programme is not very fast, it took me one hour to gather the data of 7500 vehicles with my computer.\n",
    "\\\n",
    "In the programme above, I stopped the loop after only 16 vehicles\n",
    "\\\n",
    "\\\n",
    "**Display some rows of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab186731-1402-4776-a4ce-85dbf12ee730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>href</th>\n",
       "      <th>model</th>\n",
       "      <th>version</th>\n",
       "      <th>price</th>\n",
       "      <th>dep</th>\n",
       "      <th>prof</th>\n",
       "      <th>warranty</th>\n",
       "      <th>year</th>\n",
       "      <th>km</th>\n",
       "      <th>...</th>\n",
       "      <th>Consommation mixte? :</th>\n",
       "      <th>Norme Euro? :</th>\n",
       "      <th>Prime à la conversion? :</th>\n",
       "      <th>Longueur</th>\n",
       "      <th>Volume du coffre</th>\n",
       "      <th>Ref_annonce</th>\n",
       "      <th>Nb_option</th>\n",
       "      <th>publication</th>\n",
       "      <th>Couleur intérieure :</th>\n",
       "      <th>Puissance moteur :</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.lacentrale.fr/auto-occasion-annonc...</td>\n",
       "      <td>PEUGEOT 208 (2E GENERATION)</td>\n",
       "      <td>II 1.2 PURETECH 100 S&amp;S ALLURE</td>\n",
       "      <td>24 610 €</td>\n",
       "      <td>93</td>\n",
       "      <td>Professionnel</td>\n",
       "      <td>Garantie 12 mois</td>\n",
       "      <td>2020</td>\n",
       "      <td>36 573 km</td>\n",
       "      <td>...</td>\n",
       "      <td>5.3 l/100km</td>\n",
       "      <td>EURO6</td>\n",
       "      <td></td>\n",
       "      <td>4.05 m</td>\n",
       "      <td>311 L</td>\n",
       "      <td>E111436217</td>\n",
       "      <td>9</td>\n",
       "      <td>Publié depuis : 13 jours</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.lacentrale.fr/auto-occasion-annonc...</td>\n",
       "      <td>PEUGEOT 308 (2E GENERATION)</td>\n",
       "      <td>II (2) 1.5 BLUEHDI 130 EAT8 S&amp;S ALLURE PACK</td>\n",
       "      <td>36 280 €</td>\n",
       "      <td>30</td>\n",
       "      <td>Professionnel</td>\n",
       "      <td>Garantie 12 mois</td>\n",
       "      <td>2022</td>\n",
       "      <td>12 484 km</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6 l/100km</td>\n",
       "      <td>EURO6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.25 m</td>\n",
       "      <td>420 L</td>\n",
       "      <td>E111475546</td>\n",
       "      <td>3</td>\n",
       "      <td>Publié depuis : 6 jours</td>\n",
       "      <td>Gris</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.lacentrale.fr/auto-occasion-annonc...</td>\n",
       "      <td>CITROEN C4 PICASSO 2</td>\n",
       "      <td>II 1.6 BLUEHDI 120 S&amp;S EXCLUSIVE BV6</td>\n",
       "      <td>14 700 €</td>\n",
       "      <td>95</td>\n",
       "      <td>Professionnel</td>\n",
       "      <td>Garantie 12 mois</td>\n",
       "      <td>2015</td>\n",
       "      <td>53 198 km</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5 l/100km</td>\n",
       "      <td>EURO6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.43 m</td>\n",
       "      <td>630 L</td>\n",
       "      <td>E111187957</td>\n",
       "      <td>9</td>\n",
       "      <td>Publié depuis : 60 jours</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  page                                               href  \\\n",
       "0    1  https://www.lacentrale.fr/auto-occasion-annonc...   \n",
       "1    1  https://www.lacentrale.fr/auto-occasion-annonc...   \n",
       "2    1  https://www.lacentrale.fr/auto-occasion-annonc...   \n",
       "\n",
       "                         model                                      version  \\\n",
       "0  PEUGEOT 208 (2E GENERATION)               II 1.2 PURETECH 100 S&S ALLURE   \n",
       "1  PEUGEOT 308 (2E GENERATION)  II (2) 1.5 BLUEHDI 130 EAT8 S&S ALLURE PACK   \n",
       "2         CITROEN C4 PICASSO 2         II 1.6 BLUEHDI 120 S&S EXCLUSIVE BV6   \n",
       "\n",
       "      price dep           prof           warranty  year         km  ...  \\\n",
       "0  24 610 €  93  Professionnel  Garantie 12 mois   2020  36 573 km  ...   \n",
       "1  36 280 €  30  Professionnel  Garantie 12 mois   2022  12 484 km  ...   \n",
       "2  14 700 €  95  Professionnel  Garantie 12 mois   2015  53 198 km  ...   \n",
       "\n",
       "  Consommation mixte? :  Norme Euro? :  Prime à la conversion? :  Longueur   \\\n",
       "0            5.3 l/100km          EURO6                              4.05 m   \n",
       "1            4.6 l/100km          EURO6                       NaN    4.25 m   \n",
       "2            3.5 l/100km          EURO6                       NaN    4.43 m   \n",
       "\n",
       "  Volume du coffre  Ref_annonce Nb_option                 publication  \\\n",
       "0             311 L  E111436217        9     Publié depuis : 13 jours   \n",
       "1             420 L  E111475546        3      Publié depuis : 6 jours   \n",
       "2             630 L  E111187957        9     Publié depuis : 60 jours   \n",
       "\n",
       "  Couleur intérieure :  Puissance moteur :   \n",
       "0                   NaN                 NaN  \n",
       "1                  Gris                 NaN  \n",
       "2                   NaN                 NaN  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6270c08-a056-417b-b16a-228ade6bbe68",
   "metadata": {},
   "source": [
    "**Saving the DataFrame as a csv file** \\\n",
    "I propose to add the date and hour to the file name by using datetime library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0583cdc4-0111-4b18-b24b-544e3745cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Save the finale file  #########################################\"\n",
    "file_name='lecentral_data'+datetime.datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")+'.csv'\n",
    "df.to_csv(file_name,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782a22b-bf4c-4837-8f5b-e8c0f42383d9",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This is a sample way to gother data from the web. \n",
    "as you can remark the website [**La centrale**](https://www.lacentrale.fr/) alows only 500 pages, \n",
    "each page has 15 or 16 vehicles, so we can gother a total around 7500 vehicles only. \n",
    "\\\n",
    "\\\n",
    "But the website has more than that, as we can see the example below, the website has more than 264 000 vehicles. A way to gather the data from all vehicle, is to complete the code above with a searching by batch, each time we can define automatically the maximum and minimum price to have around 7500 vehicles. We can integrate this in a loop and inside it we can loop the 500 pages of each price batch. This idea will be explained in detail in a incomping project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651dc27-42f3-45b1-b072-c087a21da4b1",
   "metadata": {},
   "source": [
    "<img src=\"img/img3.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0ae1c-f9ef-43e9-9905-f9b94ecbab63",
   "metadata": {},
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26296c1-6e41-48ca-ae22-f7779610ab72",
   "metadata": {},
   "source": [
    "As I mentioned in the conclusion you can download below the dataset of the actual programme with around 7500 vehicles, and also another dataset with more than 140 000 vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526f807-7427-4601-909a-915bd188daed",
   "metadata": {},
   "source": [
    "[**Dataset of 7412 vehicles**](https://bouz1.github.io/projects/datasets/lecentreal/lecentral_data2022_11_20__15_48_30.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ff87f8-5365-49d4-af36-f7a3acdacfe0",
   "metadata": {},
   "source": [
    "[**Dataset of 149240 vehicles**](https://bouz1.github.io/projects/datasets/lecentreal/lecentral_raspberry_database.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce9e33-03cd-4bda-8e79-f22d912047ae",
   "metadata": {},
   "source": [
    "if the links didn't work, please use the \n",
    "https://bouz1.github.io/projects/datasets/lecentreal/lecentral_data2022_11_20__15_48_30.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
